{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6d18da",
   "metadata": {
    "executionInfo": {
     "elapsed": 1572,
     "status": "ok",
     "timestamp": 1657297397735,
     "user": {
      "displayName": "Francesca Spezzano",
      "userId": "01786709092490819085"
     },
     "user_tz": 360
    },
    "id": "9a87198f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd1740b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "executionInfo": {
     "elapsed": 45587,
     "status": "ok",
     "timestamp": 1657297498179,
     "user": {
      "displayName": "Francesca Spezzano",
      "userId": "01786709092490819085"
     },
     "user_tz": 360
    },
    "id": "11a9bf62",
    "outputId": "93162e45-2a30-41aa-c692-3b4f5f8b9c1c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emo_df = pd.read_csv('recovery-news-data-emotion-features.csv')\n",
    "read_df=pd.read_csv('recovery-news-data-readability-features.csv')\n",
    "write_df=pd.read_csv('recovery-news-data-writing-features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd13779a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Objective</th>\n",
       "      <th>afraid</th>\n",
       "      <th>amused</th>\n",
       "      <th>angry</th>\n",
       "      <th>annoyed</th>\n",
       "      <th>dont_care</th>\n",
       "      <th>happy</th>\n",
       "      <th>inspired</th>\n",
       "      <th>sad</th>\n",
       "      <th>reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.061233</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.068307</td>\n",
       "      <td>0.718545</td>\n",
       "      <td>0.187126</td>\n",
       "      <td>0.052152</td>\n",
       "      <td>0.058249</td>\n",
       "      <td>0.034350</td>\n",
       "      <td>0.284639</td>\n",
       "      <td>0.254297</td>\n",
       "      <td>0.067645</td>\n",
       "      <td>0.061542</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011874</td>\n",
       "      <td>0.027143</td>\n",
       "      <td>0.017349</td>\n",
       "      <td>0.041963</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.059647</td>\n",
       "      <td>0.771184</td>\n",
       "      <td>0.145114</td>\n",
       "      <td>0.064628</td>\n",
       "      <td>0.118878</td>\n",
       "      <td>0.041245</td>\n",
       "      <td>0.333777</td>\n",
       "      <td>0.174415</td>\n",
       "      <td>0.053452</td>\n",
       "      <td>0.068490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.018336</td>\n",
       "      <td>0.037914</td>\n",
       "      <td>0.046958</td>\n",
       "      <td>0.066391</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.065443</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.055853</td>\n",
       "      <td>0.665447</td>\n",
       "      <td>0.216482</td>\n",
       "      <td>0.061477</td>\n",
       "      <td>0.063863</td>\n",
       "      <td>0.034224</td>\n",
       "      <td>0.198211</td>\n",
       "      <td>0.247231</td>\n",
       "      <td>0.080797</td>\n",
       "      <td>0.097716</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.063752</td>\n",
       "      <td>0.031115</td>\n",
       "      <td>0.032674</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.081468</td>\n",
       "      <td>0.706590</td>\n",
       "      <td>0.218814</td>\n",
       "      <td>0.044995</td>\n",
       "      <td>0.075313</td>\n",
       "      <td>0.033225</td>\n",
       "      <td>0.268769</td>\n",
       "      <td>0.223028</td>\n",
       "      <td>0.056909</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.026660</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.042772</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.020359</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.039643</td>\n",
       "      <td>0.673569</td>\n",
       "      <td>0.205156</td>\n",
       "      <td>0.049457</td>\n",
       "      <td>0.065705</td>\n",
       "      <td>0.022823</td>\n",
       "      <td>0.288402</td>\n",
       "      <td>0.219217</td>\n",
       "      <td>0.060947</td>\n",
       "      <td>0.088293</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2024</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.053275</td>\n",
       "      <td>0.016001</td>\n",
       "      <td>0.034158</td>\n",
       "      <td>0.059056</td>\n",
       "      <td>0.027101</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.706424</td>\n",
       "      <td>0.109309</td>\n",
       "      <td>0.045679</td>\n",
       "      <td>0.058745</td>\n",
       "      <td>0.033031</td>\n",
       "      <td>0.297066</td>\n",
       "      <td>0.308920</td>\n",
       "      <td>0.076011</td>\n",
       "      <td>0.071239</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>2025</td>\n",
       "      <td>0.030234</td>\n",
       "      <td>0.034679</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.057117</td>\n",
       "      <td>0.039077</td>\n",
       "      <td>0.035642</td>\n",
       "      <td>0.021987</td>\n",
       "      <td>0.067945</td>\n",
       "      <td>0.704699</td>\n",
       "      <td>0.070619</td>\n",
       "      <td>0.062367</td>\n",
       "      <td>0.056961</td>\n",
       "      <td>0.064059</td>\n",
       "      <td>0.352311</td>\n",
       "      <td>0.243119</td>\n",
       "      <td>0.085429</td>\n",
       "      <td>0.065135</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>2026</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.102877</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.023160</td>\n",
       "      <td>0.021470</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0.116851</td>\n",
       "      <td>0.677108</td>\n",
       "      <td>0.105368</td>\n",
       "      <td>0.044128</td>\n",
       "      <td>0.044088</td>\n",
       "      <td>0.078532</td>\n",
       "      <td>0.247275</td>\n",
       "      <td>0.357812</td>\n",
       "      <td>0.083971</td>\n",
       "      <td>0.038825</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>2027</td>\n",
       "      <td>0.017304</td>\n",
       "      <td>0.020901</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.013270</td>\n",
       "      <td>0.013475</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>0.810281</td>\n",
       "      <td>0.063775</td>\n",
       "      <td>0.134295</td>\n",
       "      <td>0.095691</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>0.334285</td>\n",
       "      <td>0.216420</td>\n",
       "      <td>0.078294</td>\n",
       "      <td>0.037404</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>2028</td>\n",
       "      <td>0.029043</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.053885</td>\n",
       "      <td>0.034022</td>\n",
       "      <td>0.030847</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.077948</td>\n",
       "      <td>0.698336</td>\n",
       "      <td>0.068205</td>\n",
       "      <td>0.061651</td>\n",
       "      <td>0.107278</td>\n",
       "      <td>0.070161</td>\n",
       "      <td>0.270571</td>\n",
       "      <td>0.227624</td>\n",
       "      <td>0.121315</td>\n",
       "      <td>0.073195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id     Anger  Anticipation   Disgust      Fear       Joy   Sadness  \\\n",
       "0           0  0.011846      0.040958  0.031052  0.061233  0.023925  0.039174   \n",
       "1           1  0.011874      0.027143  0.017349  0.041963  0.023242  0.037897   \n",
       "2           2  0.018336      0.037914  0.046958  0.066391  0.030701  0.065443   \n",
       "3           3  0.012988      0.045399  0.021112  0.063752  0.031115  0.032674   \n",
       "4           4  0.026660      0.033526  0.042772  0.089552  0.020359  0.068824   \n",
       "...       ...       ...           ...       ...       ...       ...       ...   \n",
       "2024     2024  0.014073      0.053275  0.016001  0.034158  0.059056  0.027101   \n",
       "2025     2025  0.030234      0.034679  0.008621  0.057117  0.039077  0.035642   \n",
       "2026     2026  0.012072      0.102877  0.005359  0.023160  0.021470  0.025328   \n",
       "2027     2027  0.017304      0.020901  0.007770  0.023442  0.013270  0.013475   \n",
       "2028     2028  0.029043      0.042088  0.018604  0.053885  0.034022  0.030847   \n",
       "\n",
       "      Surprise     Trust  Objective    afraid    amused     angry   annoyed  \\\n",
       "0     0.004960  0.068307   0.718545  0.187126  0.052152  0.058249  0.034350   \n",
       "1     0.009701  0.059647   0.771184  0.145114  0.064628  0.118878  0.041245   \n",
       "2     0.012957  0.055853   0.665447  0.216482  0.061477  0.063863  0.034224   \n",
       "3     0.004902  0.081468   0.706590  0.218814  0.044995  0.075313  0.033225   \n",
       "4     0.005095  0.039643   0.673569  0.205156  0.049457  0.065705  0.022823   \n",
       "...        ...       ...        ...       ...       ...       ...       ...   \n",
       "2024  0.010837  0.079075   0.706424  0.109309  0.045679  0.058745  0.033031   \n",
       "2025  0.021987  0.067945   0.704699  0.070619  0.062367  0.056961  0.064059   \n",
       "2026  0.015776  0.116851   0.677108  0.105368  0.044128  0.044088  0.078532   \n",
       "2027  0.006283  0.087273   0.810281  0.063775  0.134295  0.095691  0.039837   \n",
       "2028  0.015228  0.077948   0.698336  0.068205  0.061651  0.107278  0.070161   \n",
       "\n",
       "      dont_care     happy  inspired       sad  reliability  \n",
       "0      0.284639  0.254297  0.067645  0.061542          1.0  \n",
       "1      0.333777  0.174415  0.053452  0.068490          1.0  \n",
       "2      0.198211  0.247231  0.080797  0.097716          1.0  \n",
       "3      0.268769  0.223028  0.056909  0.078947          0.0  \n",
       "4      0.288402  0.219217  0.060947  0.088293          1.0  \n",
       "...         ...       ...       ...       ...          ...  \n",
       "2024   0.297066  0.308920  0.076011  0.071239          0.0  \n",
       "2025   0.352311  0.243119  0.085429  0.065135          0.0  \n",
       "2026   0.247275  0.357812  0.083971  0.038825          0.0  \n",
       "2027   0.334285  0.216420  0.078294  0.037404          0.0  \n",
       "2028   0.270571  0.227624  0.121315  0.073195          0.0  \n",
       "\n",
       "[2029 rows x 19 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e75a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>flesch_kincaid_grade_level</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>ari_index</th>\n",
       "      <th>lix_index</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>dale_chall_known_fraction</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>lexicon_count</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>87.283108</td>\n",
       "      <td>-1004.215753</td>\n",
       "      <td>412.189204</td>\n",
       "      <td>21.736222</td>\n",
       "      <td>425.522148</td>\n",
       "      <td>530.109233</td>\n",
       "      <td>1087.007670</td>\n",
       "      <td>60.861630</td>\n",
       "      <td>0.421860</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>53.258245</td>\n",
       "      <td>-439.791414</td>\n",
       "      <td>199.088805</td>\n",
       "      <td>18.940518</td>\n",
       "      <td>206.935458</td>\n",
       "      <td>257.398406</td>\n",
       "      <td>535.067729</td>\n",
       "      <td>33.517646</td>\n",
       "      <td>0.454183</td>\n",
       "      <td>877.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>79.773634</td>\n",
       "      <td>-1315.626760</td>\n",
       "      <td>535.393607</td>\n",
       "      <td>19.220051</td>\n",
       "      <td>550.878592</td>\n",
       "      <td>688.591738</td>\n",
       "      <td>1399.777126</td>\n",
       "      <td>75.341028</td>\n",
       "      <td>0.513196</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>75.390260</td>\n",
       "      <td>-709.048179</td>\n",
       "      <td>297.476011</td>\n",
       "      <td>22.622249</td>\n",
       "      <td>307.367604</td>\n",
       "      <td>382.826867</td>\n",
       "      <td>794.121821</td>\n",
       "      <td>46.922592</td>\n",
       "      <td>0.374833</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24.504239</td>\n",
       "      <td>-54.545175</td>\n",
       "      <td>48.950702</td>\n",
       "      <td>22.738509</td>\n",
       "      <td>50.512281</td>\n",
       "      <td>66.598158</td>\n",
       "      <td>159.614035</td>\n",
       "      <td>14.380453</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>214.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2024</td>\n",
       "      <td>68.264429</td>\n",
       "      <td>-757.762706</td>\n",
       "      <td>320.696728</td>\n",
       "      <td>19.720443</td>\n",
       "      <td>331.596064</td>\n",
       "      <td>413.503801</td>\n",
       "      <td>852.237392</td>\n",
       "      <td>48.676215</td>\n",
       "      <td>0.471095</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>2025</td>\n",
       "      <td>42.293715</td>\n",
       "      <td>-247.947778</td>\n",
       "      <td>125.877778</td>\n",
       "      <td>19.052095</td>\n",
       "      <td>131.968254</td>\n",
       "      <td>164.016000</td>\n",
       "      <td>350.555556</td>\n",
       "      <td>23.594190</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>519.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>2026</td>\n",
       "      <td>62.222207</td>\n",
       "      <td>-861.102992</td>\n",
       "      <td>361.450903</td>\n",
       "      <td>19.199717</td>\n",
       "      <td>372.257236</td>\n",
       "      <td>466.083993</td>\n",
       "      <td>953.167573</td>\n",
       "      <td>53.520354</td>\n",
       "      <td>0.497280</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>2027</td>\n",
       "      <td>57.923837</td>\n",
       "      <td>-374.876154</td>\n",
       "      <td>168.633173</td>\n",
       "      <td>22.766851</td>\n",
       "      <td>175.246154</td>\n",
       "      <td>217.468053</td>\n",
       "      <td>460.951923</td>\n",
       "      <td>30.654177</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>847.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>2028</td>\n",
       "      <td>82.903260</td>\n",
       "      <td>-876.263730</td>\n",
       "      <td>363.040654</td>\n",
       "      <td>21.986609</td>\n",
       "      <td>375.305998</td>\n",
       "      <td>467.312617</td>\n",
       "      <td>959.639040</td>\n",
       "      <td>54.454400</td>\n",
       "      <td>0.431843</td>\n",
       "      <td>1769.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id  smog_index  flesch_reading_ease  flesch_kincaid_grade_level  \\\n",
       "0           0   87.283108         -1004.215753                  412.189204   \n",
       "1           1   53.258245          -439.791414                  199.088805   \n",
       "2           2   79.773634         -1315.626760                  535.393607   \n",
       "3           3   75.390260          -709.048179                  297.476011   \n",
       "4           4   24.504239           -54.545175                   48.950702   \n",
       "...       ...         ...                  ...                         ...   \n",
       "2024     2024   68.264429          -757.762706                  320.696728   \n",
       "2025     2025   42.293715          -247.947778                  125.877778   \n",
       "2026     2026   62.222207          -861.102992                  361.450903   \n",
       "2027     2027   57.923837          -374.876154                  168.633173   \n",
       "2028     2028   82.903260          -876.263730                  363.040654   \n",
       "\n",
       "      coleman_liau_index  gunning_fog_index   ari_index    lix_index  \\\n",
       "0              21.736222         425.522148  530.109233  1087.007670   \n",
       "1              18.940518         206.935458  257.398406   535.067729   \n",
       "2              19.220051         550.878592  688.591738  1399.777126   \n",
       "3              22.622249         307.367604  382.826867   794.121821   \n",
       "4              22.738509          50.512281   66.598158   159.614035   \n",
       "...                  ...                ...         ...          ...   \n",
       "2024           19.720443         331.596064  413.503801   852.237392   \n",
       "2025           19.052095         131.968254  164.016000   350.555556   \n",
       "2026           19.199717         372.257236  466.083993   953.167573   \n",
       "2027           22.766851         175.246154  217.468053   460.951923   \n",
       "2028           21.986609         375.305998  467.312617   959.639040   \n",
       "\n",
       "      dale_chall_score  dale_chall_known_fraction  syllable_count  \\\n",
       "0            60.861630                   0.421860          1990.0   \n",
       "1            33.517646                   0.454183           877.0   \n",
       "2            75.341028                   0.513196          2351.0   \n",
       "3            46.922592                   0.374833          1474.0   \n",
       "4            14.380453                   0.447368           214.0   \n",
       "...                ...                        ...             ...   \n",
       "2024         48.676215                   0.471095          1439.0   \n",
       "2025         23.594190                   0.495238           519.0   \n",
       "2026         53.520354                   0.497280          1623.0   \n",
       "2027         30.654177                   0.365385           847.0   \n",
       "2028         54.454400                   0.431843          1769.0   \n",
       "\n",
       "      lexicon_count  sentence_count  \n",
       "0            1042.0             1.0  \n",
       "1             502.0             1.0  \n",
       "2            1364.0             1.0  \n",
       "3             747.0             1.0  \n",
       "4             114.0             1.0  \n",
       "...             ...             ...  \n",
       "2024          813.0             1.0  \n",
       "2025          315.0             1.0  \n",
       "2026          919.0             1.0  \n",
       "2027          416.0             1.0  \n",
       "2028          917.0             1.0  \n",
       "\n",
       "[2029 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4af95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_propernouns</th>\n",
       "      <th>num_personalnouns</th>\n",
       "      <th>num_ppssessivenouns</th>\n",
       "      <th>num_whpronoun</th>\n",
       "      <th>num_determinants</th>\n",
       "      <th>num_whdeterminants</th>\n",
       "      <th>num_cnum</th>\n",
       "      <th>num_adverb</th>\n",
       "      <th>num_interjections</th>\n",
       "      <th>num_verb</th>\n",
       "      <th>num_adj</th>\n",
       "      <th>num_vbd</th>\n",
       "      <th>num_vbg</th>\n",
       "      <th>num_vbn</th>\n",
       "      <th>num_vbp</th>\n",
       "      <th>num_vbz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>132.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>392.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>204.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2024</td>\n",
       "      <td>208.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>2025</td>\n",
       "      <td>98.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>2026</td>\n",
       "      <td>271.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>2027</td>\n",
       "      <td>120.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>2028</td>\n",
       "      <td>204.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id  num_nouns  num_propernouns  num_personalnouns  \\\n",
       "0           0      288.0            153.0               34.0   \n",
       "1           1      132.0            122.0               24.0   \n",
       "2           2      392.0            143.0               63.0   \n",
       "3           3      204.0            119.0               22.0   \n",
       "4           4       31.0             14.0                1.0   \n",
       "...       ...        ...              ...                ...   \n",
       "2024     2024      208.0            192.0               30.0   \n",
       "2025     2025       98.0             69.0               24.0   \n",
       "2026     2026      271.0            133.0               39.0   \n",
       "2027     2027      120.0             53.0               15.0   \n",
       "2028     2028      204.0            285.0               66.0   \n",
       "\n",
       "      num_ppssessivenouns  num_whpronoun  num_determinants  \\\n",
       "0                    15.0           13.0             168.0   \n",
       "1                     6.0            3.0              74.0   \n",
       "2                    14.0           15.0             269.0   \n",
       "3                    14.0            4.0             129.0   \n",
       "4                     1.0            0.0              23.0   \n",
       "...                   ...            ...               ...   \n",
       "2024                 16.0            3.0             113.0   \n",
       "2025                 12.0            2.0              51.0   \n",
       "2026                  7.0            5.0             131.0   \n",
       "2027                  4.0            3.0              57.0   \n",
       "2028                 22.0           13.0             142.0   \n",
       "\n",
       "      num_whdeterminants  num_cnum  num_adverb  num_interjections  num_verb  \\\n",
       "0                   16.0      23.0        66.0                0.0      88.0   \n",
       "1                    5.0      18.0        44.0                0.0      23.0   \n",
       "2                   18.0      43.0       118.0                0.0     127.0   \n",
       "3                    7.0      15.0        43.0                0.0      55.0   \n",
       "4                    0.0       3.0         6.0                0.0       6.0   \n",
       "...                  ...       ...         ...                ...       ...   \n",
       "2024                 6.0      24.0        48.0                0.0      57.0   \n",
       "2025                 2.0       7.0        26.0                0.0      15.0   \n",
       "2026                12.0      25.0        62.0                0.0     104.0   \n",
       "2027                 8.0      12.0        21.0                0.0      31.0   \n",
       "2028                13.0      11.0        71.0                0.0      51.0   \n",
       "\n",
       "      num_adj  num_vbd  num_vbg  num_vbn  num_vbp  num_vbz  \n",
       "0       153.0     43.0     41.0     59.0     68.0     48.0  \n",
       "1        65.0     34.0     18.0     32.0     28.0     24.0  \n",
       "2       198.0     62.0     47.0     46.0     94.0     67.0  \n",
       "3       108.0     47.0     41.0     51.0     37.0     29.0  \n",
       "4        13.0      4.0      6.0      5.0     10.0      3.0  \n",
       "...       ...      ...      ...      ...      ...      ...  \n",
       "2024    104.0     47.0     30.0     25.0     34.0     33.0  \n",
       "2025     37.0     27.0     20.0     12.0     19.0     16.0  \n",
       "2026    111.0     41.0     46.0     26.0     50.0     40.0  \n",
       "2027     72.0     21.0     22.0     16.0     19.0     25.0  \n",
       "2028    120.0     24.0     35.0     34.0     77.0     57.0  \n",
       "\n",
       "[2029 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "622ef343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_propernouns</th>\n",
       "      <th>num_personalnouns</th>\n",
       "      <th>num_ppssessivenouns</th>\n",
       "      <th>num_whpronoun</th>\n",
       "      <th>num_determinants</th>\n",
       "      <th>num_whdeterminants</th>\n",
       "      <th>num_cnum</th>\n",
       "      <th>num_adverb</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch_kincaid_grade_level</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>ari_index</th>\n",
       "      <th>lix_index</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>dale_chall_known_fraction</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>lexicon_count</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>412.189204</td>\n",
       "      <td>21.736222</td>\n",
       "      <td>425.522148</td>\n",
       "      <td>530.109233</td>\n",
       "      <td>1087.007670</td>\n",
       "      <td>60.861630</td>\n",
       "      <td>0.421860</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>132.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>199.088805</td>\n",
       "      <td>18.940518</td>\n",
       "      <td>206.935458</td>\n",
       "      <td>257.398406</td>\n",
       "      <td>535.067729</td>\n",
       "      <td>33.517646</td>\n",
       "      <td>0.454183</td>\n",
       "      <td>877.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>392.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>535.393607</td>\n",
       "      <td>19.220051</td>\n",
       "      <td>550.878592</td>\n",
       "      <td>688.591738</td>\n",
       "      <td>1399.777126</td>\n",
       "      <td>75.341028</td>\n",
       "      <td>0.513196</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>204.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>297.476011</td>\n",
       "      <td>22.622249</td>\n",
       "      <td>307.367604</td>\n",
       "      <td>382.826867</td>\n",
       "      <td>794.121821</td>\n",
       "      <td>46.922592</td>\n",
       "      <td>0.374833</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.950702</td>\n",
       "      <td>22.738509</td>\n",
       "      <td>50.512281</td>\n",
       "      <td>66.598158</td>\n",
       "      <td>159.614035</td>\n",
       "      <td>14.380453</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>214.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2024</td>\n",
       "      <td>208.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>320.696728</td>\n",
       "      <td>19.720443</td>\n",
       "      <td>331.596064</td>\n",
       "      <td>413.503801</td>\n",
       "      <td>852.237392</td>\n",
       "      <td>48.676215</td>\n",
       "      <td>0.471095</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>2025</td>\n",
       "      <td>98.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>125.877778</td>\n",
       "      <td>19.052095</td>\n",
       "      <td>131.968254</td>\n",
       "      <td>164.016000</td>\n",
       "      <td>350.555556</td>\n",
       "      <td>23.594190</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>519.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>2026</td>\n",
       "      <td>271.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>...</td>\n",
       "      <td>361.450903</td>\n",
       "      <td>19.199717</td>\n",
       "      <td>372.257236</td>\n",
       "      <td>466.083993</td>\n",
       "      <td>953.167573</td>\n",
       "      <td>53.520354</td>\n",
       "      <td>0.497280</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>2027</td>\n",
       "      <td>120.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>168.633173</td>\n",
       "      <td>22.766851</td>\n",
       "      <td>175.246154</td>\n",
       "      <td>217.468053</td>\n",
       "      <td>460.951923</td>\n",
       "      <td>30.654177</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>847.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>2028</td>\n",
       "      <td>204.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>363.040654</td>\n",
       "      <td>21.986609</td>\n",
       "      <td>375.305998</td>\n",
       "      <td>467.312617</td>\n",
       "      <td>959.639040</td>\n",
       "      <td>54.454400</td>\n",
       "      <td>0.431843</td>\n",
       "      <td>1769.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id  num_nouns  num_propernouns  num_personalnouns  \\\n",
       "0           0      288.0            153.0               34.0   \n",
       "1           1      132.0            122.0               24.0   \n",
       "2           2      392.0            143.0               63.0   \n",
       "3           3      204.0            119.0               22.0   \n",
       "4           4       31.0             14.0                1.0   \n",
       "...       ...        ...              ...                ...   \n",
       "2024     2024      208.0            192.0               30.0   \n",
       "2025     2025       98.0             69.0               24.0   \n",
       "2026     2026      271.0            133.0               39.0   \n",
       "2027     2027      120.0             53.0               15.0   \n",
       "2028     2028      204.0            285.0               66.0   \n",
       "\n",
       "      num_ppssessivenouns  num_whpronoun  num_determinants  \\\n",
       "0                    15.0           13.0             168.0   \n",
       "1                     6.0            3.0              74.0   \n",
       "2                    14.0           15.0             269.0   \n",
       "3                    14.0            4.0             129.0   \n",
       "4                     1.0            0.0              23.0   \n",
       "...                   ...            ...               ...   \n",
       "2024                 16.0            3.0             113.0   \n",
       "2025                 12.0            2.0              51.0   \n",
       "2026                  7.0            5.0             131.0   \n",
       "2027                  4.0            3.0              57.0   \n",
       "2028                 22.0           13.0             142.0   \n",
       "\n",
       "      num_whdeterminants  num_cnum  num_adverb  ...  \\\n",
       "0                   16.0      23.0        66.0  ...   \n",
       "1                    5.0      18.0        44.0  ...   \n",
       "2                   18.0      43.0       118.0  ...   \n",
       "3                    7.0      15.0        43.0  ...   \n",
       "4                    0.0       3.0         6.0  ...   \n",
       "...                  ...       ...         ...  ...   \n",
       "2024                 6.0      24.0        48.0  ...   \n",
       "2025                 2.0       7.0        26.0  ...   \n",
       "2026                12.0      25.0        62.0  ...   \n",
       "2027                 8.0      12.0        21.0  ...   \n",
       "2028                13.0      11.0        71.0  ...   \n",
       "\n",
       "      flesch_kincaid_grade_level  coleman_liau_index  gunning_fog_index  \\\n",
       "0                     412.189204           21.736222         425.522148   \n",
       "1                     199.088805           18.940518         206.935458   \n",
       "2                     535.393607           19.220051         550.878592   \n",
       "3                     297.476011           22.622249         307.367604   \n",
       "4                      48.950702           22.738509          50.512281   \n",
       "...                          ...                 ...                ...   \n",
       "2024                  320.696728           19.720443         331.596064   \n",
       "2025                  125.877778           19.052095         131.968254   \n",
       "2026                  361.450903           19.199717         372.257236   \n",
       "2027                  168.633173           22.766851         175.246154   \n",
       "2028                  363.040654           21.986609         375.305998   \n",
       "\n",
       "       ari_index    lix_index  dale_chall_score  dale_chall_known_fraction  \\\n",
       "0     530.109233  1087.007670         60.861630                   0.421860   \n",
       "1     257.398406   535.067729         33.517646                   0.454183   \n",
       "2     688.591738  1399.777126         75.341028                   0.513196   \n",
       "3     382.826867   794.121821         46.922592                   0.374833   \n",
       "4      66.598158   159.614035         14.380453                   0.447368   \n",
       "...          ...          ...               ...                        ...   \n",
       "2024  413.503801   852.237392         48.676215                   0.471095   \n",
       "2025  164.016000   350.555556         23.594190                   0.495238   \n",
       "2026  466.083993   953.167573         53.520354                   0.497280   \n",
       "2027  217.468053   460.951923         30.654177                   0.365385   \n",
       "2028  467.312617   959.639040         54.454400                   0.431843   \n",
       "\n",
       "      syllable_count  lexicon_count  sentence_count  \n",
       "0             1990.0         1042.0             1.0  \n",
       "1              877.0          502.0             1.0  \n",
       "2             2351.0         1364.0             1.0  \n",
       "3             1474.0          747.0             1.0  \n",
       "4              214.0          114.0             1.0  \n",
       "...              ...            ...             ...  \n",
       "2024          1439.0          813.0             1.0  \n",
       "2025           519.0          315.0             1.0  \n",
       "2026          1623.0          919.0             1.0  \n",
       "2027           847.0          416.0             1.0  \n",
       "2028          1769.0          917.0             1.0  \n",
       "\n",
       "[2029 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([write_df, read_df], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd80257",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result[['num_nouns', 'num_propernouns', 'num_personalnouns',\n",
    "       'num_ppssessivenouns', 'num_whpronoun', 'num_determinants',\n",
    "       'num_whdeterminants', 'num_cnum', 'num_adverb', 'num_interjections',\n",
    "       'num_verb', 'num_adj', 'num_vbd', 'num_vbg', 'num_vbn', 'num_vbp',\n",
    "       'num_vbz', 'smog_index', 'flesch_reading_ease',\n",
    "       'flesch_kincaid_grade_level', 'coleman_liau_index', 'gunning_fog_index',\n",
    "       'ari_index', 'lix_index', 'dale_chall_score',\n",
    "       'dale_chall_known_fraction', 'syllable_count', 'lexicon_count',\n",
    "       'sentence_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a6907ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result=pd.concat([emo_df, result], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70776746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Objective</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch_kincaid_grade_level</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>ari_index</th>\n",
       "      <th>lix_index</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>dale_chall_known_fraction</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>lexicon_count</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.061233</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.068307</td>\n",
       "      <td>0.718545</td>\n",
       "      <td>...</td>\n",
       "      <td>412.189204</td>\n",
       "      <td>21.736222</td>\n",
       "      <td>425.522148</td>\n",
       "      <td>530.109233</td>\n",
       "      <td>1087.007670</td>\n",
       "      <td>60.861630</td>\n",
       "      <td>0.421860</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011874</td>\n",
       "      <td>0.027143</td>\n",
       "      <td>0.017349</td>\n",
       "      <td>0.041963</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.059647</td>\n",
       "      <td>0.771184</td>\n",
       "      <td>...</td>\n",
       "      <td>199.088805</td>\n",
       "      <td>18.940518</td>\n",
       "      <td>206.935458</td>\n",
       "      <td>257.398406</td>\n",
       "      <td>535.067729</td>\n",
       "      <td>33.517646</td>\n",
       "      <td>0.454183</td>\n",
       "      <td>877.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.018336</td>\n",
       "      <td>0.037914</td>\n",
       "      <td>0.046958</td>\n",
       "      <td>0.066391</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.065443</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.055853</td>\n",
       "      <td>0.665447</td>\n",
       "      <td>...</td>\n",
       "      <td>535.393607</td>\n",
       "      <td>19.220051</td>\n",
       "      <td>550.878592</td>\n",
       "      <td>688.591738</td>\n",
       "      <td>1399.777126</td>\n",
       "      <td>75.341028</td>\n",
       "      <td>0.513196</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.063752</td>\n",
       "      <td>0.031115</td>\n",
       "      <td>0.032674</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.081468</td>\n",
       "      <td>0.706590</td>\n",
       "      <td>...</td>\n",
       "      <td>297.476011</td>\n",
       "      <td>22.622249</td>\n",
       "      <td>307.367604</td>\n",
       "      <td>382.826867</td>\n",
       "      <td>794.121821</td>\n",
       "      <td>46.922592</td>\n",
       "      <td>0.374833</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.026660</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.042772</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.020359</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.039643</td>\n",
       "      <td>0.673569</td>\n",
       "      <td>...</td>\n",
       "      <td>48.950702</td>\n",
       "      <td>22.738509</td>\n",
       "      <td>50.512281</td>\n",
       "      <td>66.598158</td>\n",
       "      <td>159.614035</td>\n",
       "      <td>14.380453</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>214.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2024</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.053275</td>\n",
       "      <td>0.016001</td>\n",
       "      <td>0.034158</td>\n",
       "      <td>0.059056</td>\n",
       "      <td>0.027101</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.706424</td>\n",
       "      <td>...</td>\n",
       "      <td>320.696728</td>\n",
       "      <td>19.720443</td>\n",
       "      <td>331.596064</td>\n",
       "      <td>413.503801</td>\n",
       "      <td>852.237392</td>\n",
       "      <td>48.676215</td>\n",
       "      <td>0.471095</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>2025</td>\n",
       "      <td>0.030234</td>\n",
       "      <td>0.034679</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.057117</td>\n",
       "      <td>0.039077</td>\n",
       "      <td>0.035642</td>\n",
       "      <td>0.021987</td>\n",
       "      <td>0.067945</td>\n",
       "      <td>0.704699</td>\n",
       "      <td>...</td>\n",
       "      <td>125.877778</td>\n",
       "      <td>19.052095</td>\n",
       "      <td>131.968254</td>\n",
       "      <td>164.016000</td>\n",
       "      <td>350.555556</td>\n",
       "      <td>23.594190</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>519.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>2026</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.102877</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.023160</td>\n",
       "      <td>0.021470</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0.116851</td>\n",
       "      <td>0.677108</td>\n",
       "      <td>...</td>\n",
       "      <td>361.450903</td>\n",
       "      <td>19.199717</td>\n",
       "      <td>372.257236</td>\n",
       "      <td>466.083993</td>\n",
       "      <td>953.167573</td>\n",
       "      <td>53.520354</td>\n",
       "      <td>0.497280</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>2027</td>\n",
       "      <td>0.017304</td>\n",
       "      <td>0.020901</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.013270</td>\n",
       "      <td>0.013475</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>0.810281</td>\n",
       "      <td>...</td>\n",
       "      <td>168.633173</td>\n",
       "      <td>22.766851</td>\n",
       "      <td>175.246154</td>\n",
       "      <td>217.468053</td>\n",
       "      <td>460.951923</td>\n",
       "      <td>30.654177</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>847.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>2028</td>\n",
       "      <td>0.029043</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.053885</td>\n",
       "      <td>0.034022</td>\n",
       "      <td>0.030847</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.077948</td>\n",
       "      <td>0.698336</td>\n",
       "      <td>...</td>\n",
       "      <td>363.040654</td>\n",
       "      <td>21.986609</td>\n",
       "      <td>375.305998</td>\n",
       "      <td>467.312617</td>\n",
       "      <td>959.639040</td>\n",
       "      <td>54.454400</td>\n",
       "      <td>0.431843</td>\n",
       "      <td>1769.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id     Anger  Anticipation   Disgust      Fear       Joy   Sadness  \\\n",
       "0           0  0.011846      0.040958  0.031052  0.061233  0.023925  0.039174   \n",
       "1           1  0.011874      0.027143  0.017349  0.041963  0.023242  0.037897   \n",
       "2           2  0.018336      0.037914  0.046958  0.066391  0.030701  0.065443   \n",
       "3           3  0.012988      0.045399  0.021112  0.063752  0.031115  0.032674   \n",
       "4           4  0.026660      0.033526  0.042772  0.089552  0.020359  0.068824   \n",
       "...       ...       ...           ...       ...       ...       ...       ...   \n",
       "2024     2024  0.014073      0.053275  0.016001  0.034158  0.059056  0.027101   \n",
       "2025     2025  0.030234      0.034679  0.008621  0.057117  0.039077  0.035642   \n",
       "2026     2026  0.012072      0.102877  0.005359  0.023160  0.021470  0.025328   \n",
       "2027     2027  0.017304      0.020901  0.007770  0.023442  0.013270  0.013475   \n",
       "2028     2028  0.029043      0.042088  0.018604  0.053885  0.034022  0.030847   \n",
       "\n",
       "      Surprise     Trust  Objective  ...  flesch_kincaid_grade_level  \\\n",
       "0     0.004960  0.068307   0.718545  ...                  412.189204   \n",
       "1     0.009701  0.059647   0.771184  ...                  199.088805   \n",
       "2     0.012957  0.055853   0.665447  ...                  535.393607   \n",
       "3     0.004902  0.081468   0.706590  ...                  297.476011   \n",
       "4     0.005095  0.039643   0.673569  ...                   48.950702   \n",
       "...        ...       ...        ...  ...                         ...   \n",
       "2024  0.010837  0.079075   0.706424  ...                  320.696728   \n",
       "2025  0.021987  0.067945   0.704699  ...                  125.877778   \n",
       "2026  0.015776  0.116851   0.677108  ...                  361.450903   \n",
       "2027  0.006283  0.087273   0.810281  ...                  168.633173   \n",
       "2028  0.015228  0.077948   0.698336  ...                  363.040654   \n",
       "\n",
       "      coleman_liau_index  gunning_fog_index   ari_index    lix_index  \\\n",
       "0              21.736222         425.522148  530.109233  1087.007670   \n",
       "1              18.940518         206.935458  257.398406   535.067729   \n",
       "2              19.220051         550.878592  688.591738  1399.777126   \n",
       "3              22.622249         307.367604  382.826867   794.121821   \n",
       "4              22.738509          50.512281   66.598158   159.614035   \n",
       "...                  ...                ...         ...          ...   \n",
       "2024           19.720443         331.596064  413.503801   852.237392   \n",
       "2025           19.052095         131.968254  164.016000   350.555556   \n",
       "2026           19.199717         372.257236  466.083993   953.167573   \n",
       "2027           22.766851         175.246154  217.468053   460.951923   \n",
       "2028           21.986609         375.305998  467.312617   959.639040   \n",
       "\n",
       "      dale_chall_score  dale_chall_known_fraction  syllable_count  \\\n",
       "0            60.861630                   0.421860          1990.0   \n",
       "1            33.517646                   0.454183           877.0   \n",
       "2            75.341028                   0.513196          2351.0   \n",
       "3            46.922592                   0.374833          1474.0   \n",
       "4            14.380453                   0.447368           214.0   \n",
       "...                ...                        ...             ...   \n",
       "2024         48.676215                   0.471095          1439.0   \n",
       "2025         23.594190                   0.495238           519.0   \n",
       "2026         53.520354                   0.497280          1623.0   \n",
       "2027         30.654177                   0.365385           847.0   \n",
       "2028         54.454400                   0.431843          1769.0   \n",
       "\n",
       "      lexicon_count  sentence_count  \n",
       "0            1042.0             1.0  \n",
       "1             502.0             1.0  \n",
       "2            1364.0             1.0  \n",
       "3             747.0             1.0  \n",
       "4             114.0             1.0  \n",
       "...             ...             ...  \n",
       "2024          813.0             1.0  \n",
       "2025          315.0             1.0  \n",
       "2026          919.0             1.0  \n",
       "2027          416.0             1.0  \n",
       "2028          917.0             1.0  \n",
       "\n",
       "[2029 rows x 48 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "619816b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result=final_result[['news_id', 'Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Sadness',\n",
    "       'Surprise', 'Trust', 'Objective', 'afraid', 'amused', 'angry',\n",
    "       'annoyed', 'dont_care', 'happy', 'inspired', 'sad',\n",
    "       'num_nouns', 'num_propernouns', 'num_personalnouns',\n",
    "       'num_ppssessivenouns', 'num_whpronoun', 'num_determinants',\n",
    "       'num_whdeterminants', 'num_cnum', 'num_adverb', 'num_interjections',\n",
    "       'num_verb', 'num_adj', 'num_vbd', 'num_vbg', 'num_vbn', 'num_vbp',\n",
    "       'num_vbz', 'smog_index', 'flesch_reading_ease',\n",
    "       'flesch_kincaid_grade_level', 'coleman_liau_index', 'gunning_fog_index',\n",
    "       'ari_index', 'lix_index', 'dale_chall_score',\n",
    "       'dale_chall_known_fraction', 'syllable_count', 'lexicon_count',\n",
    "       'sentence_count', 'reliability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdf7d213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Objective</th>\n",
       "      <th>...</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>ari_index</th>\n",
       "      <th>lix_index</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>dale_chall_known_fraction</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>lexicon_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.061233</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.068307</td>\n",
       "      <td>0.718545</td>\n",
       "      <td>...</td>\n",
       "      <td>21.736222</td>\n",
       "      <td>425.522148</td>\n",
       "      <td>530.109233</td>\n",
       "      <td>1087.007670</td>\n",
       "      <td>60.861630</td>\n",
       "      <td>0.421860</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011874</td>\n",
       "      <td>0.027143</td>\n",
       "      <td>0.017349</td>\n",
       "      <td>0.041963</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.059647</td>\n",
       "      <td>0.771184</td>\n",
       "      <td>...</td>\n",
       "      <td>18.940518</td>\n",
       "      <td>206.935458</td>\n",
       "      <td>257.398406</td>\n",
       "      <td>535.067729</td>\n",
       "      <td>33.517646</td>\n",
       "      <td>0.454183</td>\n",
       "      <td>877.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.018336</td>\n",
       "      <td>0.037914</td>\n",
       "      <td>0.046958</td>\n",
       "      <td>0.066391</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.065443</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.055853</td>\n",
       "      <td>0.665447</td>\n",
       "      <td>...</td>\n",
       "      <td>19.220051</td>\n",
       "      <td>550.878592</td>\n",
       "      <td>688.591738</td>\n",
       "      <td>1399.777126</td>\n",
       "      <td>75.341028</td>\n",
       "      <td>0.513196</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.063752</td>\n",
       "      <td>0.031115</td>\n",
       "      <td>0.032674</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.081468</td>\n",
       "      <td>0.706590</td>\n",
       "      <td>...</td>\n",
       "      <td>22.622249</td>\n",
       "      <td>307.367604</td>\n",
       "      <td>382.826867</td>\n",
       "      <td>794.121821</td>\n",
       "      <td>46.922592</td>\n",
       "      <td>0.374833</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.026660</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.042772</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.020359</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.039643</td>\n",
       "      <td>0.673569</td>\n",
       "      <td>...</td>\n",
       "      <td>22.738509</td>\n",
       "      <td>50.512281</td>\n",
       "      <td>66.598158</td>\n",
       "      <td>159.614035</td>\n",
       "      <td>14.380453</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>214.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2024</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.053275</td>\n",
       "      <td>0.016001</td>\n",
       "      <td>0.034158</td>\n",
       "      <td>0.059056</td>\n",
       "      <td>0.027101</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.706424</td>\n",
       "      <td>...</td>\n",
       "      <td>19.720443</td>\n",
       "      <td>331.596064</td>\n",
       "      <td>413.503801</td>\n",
       "      <td>852.237392</td>\n",
       "      <td>48.676215</td>\n",
       "      <td>0.471095</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>2025</td>\n",
       "      <td>0.030234</td>\n",
       "      <td>0.034679</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.057117</td>\n",
       "      <td>0.039077</td>\n",
       "      <td>0.035642</td>\n",
       "      <td>0.021987</td>\n",
       "      <td>0.067945</td>\n",
       "      <td>0.704699</td>\n",
       "      <td>...</td>\n",
       "      <td>19.052095</td>\n",
       "      <td>131.968254</td>\n",
       "      <td>164.016000</td>\n",
       "      <td>350.555556</td>\n",
       "      <td>23.594190</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>519.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>2026</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.102877</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.023160</td>\n",
       "      <td>0.021470</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0.116851</td>\n",
       "      <td>0.677108</td>\n",
       "      <td>...</td>\n",
       "      <td>19.199717</td>\n",
       "      <td>372.257236</td>\n",
       "      <td>466.083993</td>\n",
       "      <td>953.167573</td>\n",
       "      <td>53.520354</td>\n",
       "      <td>0.497280</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>2027</td>\n",
       "      <td>0.017304</td>\n",
       "      <td>0.020901</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.013270</td>\n",
       "      <td>0.013475</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>0.810281</td>\n",
       "      <td>...</td>\n",
       "      <td>22.766851</td>\n",
       "      <td>175.246154</td>\n",
       "      <td>217.468053</td>\n",
       "      <td>460.951923</td>\n",
       "      <td>30.654177</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>847.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>2028</td>\n",
       "      <td>0.029043</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.053885</td>\n",
       "      <td>0.034022</td>\n",
       "      <td>0.030847</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.077948</td>\n",
       "      <td>0.698336</td>\n",
       "      <td>...</td>\n",
       "      <td>21.986609</td>\n",
       "      <td>375.305998</td>\n",
       "      <td>467.312617</td>\n",
       "      <td>959.639040</td>\n",
       "      <td>54.454400</td>\n",
       "      <td>0.431843</td>\n",
       "      <td>1769.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id     Anger  Anticipation   Disgust      Fear       Joy   Sadness  \\\n",
       "0           0  0.011846      0.040958  0.031052  0.061233  0.023925  0.039174   \n",
       "1           1  0.011874      0.027143  0.017349  0.041963  0.023242  0.037897   \n",
       "2           2  0.018336      0.037914  0.046958  0.066391  0.030701  0.065443   \n",
       "3           3  0.012988      0.045399  0.021112  0.063752  0.031115  0.032674   \n",
       "4           4  0.026660      0.033526  0.042772  0.089552  0.020359  0.068824   \n",
       "...       ...       ...           ...       ...       ...       ...       ...   \n",
       "2024     2024  0.014073      0.053275  0.016001  0.034158  0.059056  0.027101   \n",
       "2025     2025  0.030234      0.034679  0.008621  0.057117  0.039077  0.035642   \n",
       "2026     2026  0.012072      0.102877  0.005359  0.023160  0.021470  0.025328   \n",
       "2027     2027  0.017304      0.020901  0.007770  0.023442  0.013270  0.013475   \n",
       "2028     2028  0.029043      0.042088  0.018604  0.053885  0.034022  0.030847   \n",
       "\n",
       "      Surprise     Trust  Objective  ...  coleman_liau_index  \\\n",
       "0     0.004960  0.068307   0.718545  ...           21.736222   \n",
       "1     0.009701  0.059647   0.771184  ...           18.940518   \n",
       "2     0.012957  0.055853   0.665447  ...           19.220051   \n",
       "3     0.004902  0.081468   0.706590  ...           22.622249   \n",
       "4     0.005095  0.039643   0.673569  ...           22.738509   \n",
       "...        ...       ...        ...  ...                 ...   \n",
       "2024  0.010837  0.079075   0.706424  ...           19.720443   \n",
       "2025  0.021987  0.067945   0.704699  ...           19.052095   \n",
       "2026  0.015776  0.116851   0.677108  ...           19.199717   \n",
       "2027  0.006283  0.087273   0.810281  ...           22.766851   \n",
       "2028  0.015228  0.077948   0.698336  ...           21.986609   \n",
       "\n",
       "      gunning_fog_index   ari_index    lix_index  dale_chall_score  \\\n",
       "0            425.522148  530.109233  1087.007670         60.861630   \n",
       "1            206.935458  257.398406   535.067729         33.517646   \n",
       "2            550.878592  688.591738  1399.777126         75.341028   \n",
       "3            307.367604  382.826867   794.121821         46.922592   \n",
       "4             50.512281   66.598158   159.614035         14.380453   \n",
       "...                 ...         ...          ...               ...   \n",
       "2024         331.596064  413.503801   852.237392         48.676215   \n",
       "2025         131.968254  164.016000   350.555556         23.594190   \n",
       "2026         372.257236  466.083993   953.167573         53.520354   \n",
       "2027         175.246154  217.468053   460.951923         30.654177   \n",
       "2028         375.305998  467.312617   959.639040         54.454400   \n",
       "\n",
       "      dale_chall_known_fraction  syllable_count  lexicon_count  \\\n",
       "0                      0.421860          1990.0         1042.0   \n",
       "1                      0.454183           877.0          502.0   \n",
       "2                      0.513196          2351.0         1364.0   \n",
       "3                      0.374833          1474.0          747.0   \n",
       "4                      0.447368           214.0          114.0   \n",
       "...                         ...             ...            ...   \n",
       "2024                   0.471095          1439.0          813.0   \n",
       "2025                   0.495238           519.0          315.0   \n",
       "2026                   0.497280          1623.0          919.0   \n",
       "2027                   0.365385           847.0          416.0   \n",
       "2028                   0.431843          1769.0          917.0   \n",
       "\n",
       "      sentence_count  reliability  \n",
       "0                1.0          1.0  \n",
       "1                1.0          1.0  \n",
       "2                1.0          1.0  \n",
       "3                1.0          0.0  \n",
       "4                1.0          1.0  \n",
       "...              ...          ...  \n",
       "2024             1.0          0.0  \n",
       "2025             1.0          0.0  \n",
       "2026             1.0          0.0  \n",
       "2027             1.0          0.0  \n",
       "2028             1.0          0.0  \n",
       "\n",
       "[2029 rows x 48 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b9f3a8a",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1657297498490,
     "user": {
      "displayName": "Francesca Spezzano",
      "userId": "01786709092490819085"
     },
     "user_tz": 360
    },
    "id": "zaQkQE8mloVC"
   },
   "outputs": [],
   "source": [
    "df_final=final_result.drop(columns=['news_id','reliability'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e77c2415",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1657297498491,
     "user": {
      "displayName": "Francesca Spezzano",
      "userId": "01786709092490819085"
     },
     "user_tz": 360
    },
    "id": "847aa469",
    "outputId": "de2e68cc-d90c-4a02-fcdb-da33350ee94c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Objective</th>\n",
       "      <th>afraid</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch_kincaid_grade_level</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>ari_index</th>\n",
       "      <th>lix_index</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>dale_chall_known_fraction</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>lexicon_count</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.061233</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.068307</td>\n",
       "      <td>0.718545</td>\n",
       "      <td>0.187126</td>\n",
       "      <td>...</td>\n",
       "      <td>412.189204</td>\n",
       "      <td>21.736222</td>\n",
       "      <td>425.522148</td>\n",
       "      <td>530.109233</td>\n",
       "      <td>1087.007670</td>\n",
       "      <td>60.861630</td>\n",
       "      <td>0.421860</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011874</td>\n",
       "      <td>0.027143</td>\n",
       "      <td>0.017349</td>\n",
       "      <td>0.041963</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.059647</td>\n",
       "      <td>0.771184</td>\n",
       "      <td>0.145114</td>\n",
       "      <td>...</td>\n",
       "      <td>199.088805</td>\n",
       "      <td>18.940518</td>\n",
       "      <td>206.935458</td>\n",
       "      <td>257.398406</td>\n",
       "      <td>535.067729</td>\n",
       "      <td>33.517646</td>\n",
       "      <td>0.454183</td>\n",
       "      <td>877.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018336</td>\n",
       "      <td>0.037914</td>\n",
       "      <td>0.046958</td>\n",
       "      <td>0.066391</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.065443</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.055853</td>\n",
       "      <td>0.665447</td>\n",
       "      <td>0.216482</td>\n",
       "      <td>...</td>\n",
       "      <td>535.393607</td>\n",
       "      <td>19.220051</td>\n",
       "      <td>550.878592</td>\n",
       "      <td>688.591738</td>\n",
       "      <td>1399.777126</td>\n",
       "      <td>75.341028</td>\n",
       "      <td>0.513196</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.063752</td>\n",
       "      <td>0.031115</td>\n",
       "      <td>0.032674</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.081468</td>\n",
       "      <td>0.706590</td>\n",
       "      <td>0.218814</td>\n",
       "      <td>...</td>\n",
       "      <td>297.476011</td>\n",
       "      <td>22.622249</td>\n",
       "      <td>307.367604</td>\n",
       "      <td>382.826867</td>\n",
       "      <td>794.121821</td>\n",
       "      <td>46.922592</td>\n",
       "      <td>0.374833</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026660</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.042772</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.020359</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.039643</td>\n",
       "      <td>0.673569</td>\n",
       "      <td>0.205156</td>\n",
       "      <td>...</td>\n",
       "      <td>48.950702</td>\n",
       "      <td>22.738509</td>\n",
       "      <td>50.512281</td>\n",
       "      <td>66.598158</td>\n",
       "      <td>159.614035</td>\n",
       "      <td>14.380453</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>214.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.053275</td>\n",
       "      <td>0.016001</td>\n",
       "      <td>0.034158</td>\n",
       "      <td>0.059056</td>\n",
       "      <td>0.027101</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.706424</td>\n",
       "      <td>0.109309</td>\n",
       "      <td>...</td>\n",
       "      <td>320.696728</td>\n",
       "      <td>19.720443</td>\n",
       "      <td>331.596064</td>\n",
       "      <td>413.503801</td>\n",
       "      <td>852.237392</td>\n",
       "      <td>48.676215</td>\n",
       "      <td>0.471095</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>0.030234</td>\n",
       "      <td>0.034679</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.057117</td>\n",
       "      <td>0.039077</td>\n",
       "      <td>0.035642</td>\n",
       "      <td>0.021987</td>\n",
       "      <td>0.067945</td>\n",
       "      <td>0.704699</td>\n",
       "      <td>0.070619</td>\n",
       "      <td>...</td>\n",
       "      <td>125.877778</td>\n",
       "      <td>19.052095</td>\n",
       "      <td>131.968254</td>\n",
       "      <td>164.016000</td>\n",
       "      <td>350.555556</td>\n",
       "      <td>23.594190</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>519.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.102877</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.023160</td>\n",
       "      <td>0.021470</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0.116851</td>\n",
       "      <td>0.677108</td>\n",
       "      <td>0.105368</td>\n",
       "      <td>...</td>\n",
       "      <td>361.450903</td>\n",
       "      <td>19.199717</td>\n",
       "      <td>372.257236</td>\n",
       "      <td>466.083993</td>\n",
       "      <td>953.167573</td>\n",
       "      <td>53.520354</td>\n",
       "      <td>0.497280</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>0.017304</td>\n",
       "      <td>0.020901</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.013270</td>\n",
       "      <td>0.013475</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>0.810281</td>\n",
       "      <td>0.063775</td>\n",
       "      <td>...</td>\n",
       "      <td>168.633173</td>\n",
       "      <td>22.766851</td>\n",
       "      <td>175.246154</td>\n",
       "      <td>217.468053</td>\n",
       "      <td>460.951923</td>\n",
       "      <td>30.654177</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>847.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>0.029043</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.053885</td>\n",
       "      <td>0.034022</td>\n",
       "      <td>0.030847</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.077948</td>\n",
       "      <td>0.698336</td>\n",
       "      <td>0.068205</td>\n",
       "      <td>...</td>\n",
       "      <td>363.040654</td>\n",
       "      <td>21.986609</td>\n",
       "      <td>375.305998</td>\n",
       "      <td>467.312617</td>\n",
       "      <td>959.639040</td>\n",
       "      <td>54.454400</td>\n",
       "      <td>0.431843</td>\n",
       "      <td>1769.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Anger  Anticipation   Disgust      Fear       Joy   Sadness  \\\n",
       "0     0.011846      0.040958  0.031052  0.061233  0.023925  0.039174   \n",
       "1     0.011874      0.027143  0.017349  0.041963  0.023242  0.037897   \n",
       "2     0.018336      0.037914  0.046958  0.066391  0.030701  0.065443   \n",
       "3     0.012988      0.045399  0.021112  0.063752  0.031115  0.032674   \n",
       "4     0.026660      0.033526  0.042772  0.089552  0.020359  0.068824   \n",
       "...        ...           ...       ...       ...       ...       ...   \n",
       "2024  0.014073      0.053275  0.016001  0.034158  0.059056  0.027101   \n",
       "2025  0.030234      0.034679  0.008621  0.057117  0.039077  0.035642   \n",
       "2026  0.012072      0.102877  0.005359  0.023160  0.021470  0.025328   \n",
       "2027  0.017304      0.020901  0.007770  0.023442  0.013270  0.013475   \n",
       "2028  0.029043      0.042088  0.018604  0.053885  0.034022  0.030847   \n",
       "\n",
       "      Surprise     Trust  Objective    afraid  ...  \\\n",
       "0     0.004960  0.068307   0.718545  0.187126  ...   \n",
       "1     0.009701  0.059647   0.771184  0.145114  ...   \n",
       "2     0.012957  0.055853   0.665447  0.216482  ...   \n",
       "3     0.004902  0.081468   0.706590  0.218814  ...   \n",
       "4     0.005095  0.039643   0.673569  0.205156  ...   \n",
       "...        ...       ...        ...       ...  ...   \n",
       "2024  0.010837  0.079075   0.706424  0.109309  ...   \n",
       "2025  0.021987  0.067945   0.704699  0.070619  ...   \n",
       "2026  0.015776  0.116851   0.677108  0.105368  ...   \n",
       "2027  0.006283  0.087273   0.810281  0.063775  ...   \n",
       "2028  0.015228  0.077948   0.698336  0.068205  ...   \n",
       "\n",
       "      flesch_kincaid_grade_level  coleman_liau_index  gunning_fog_index  \\\n",
       "0                     412.189204           21.736222         425.522148   \n",
       "1                     199.088805           18.940518         206.935458   \n",
       "2                     535.393607           19.220051         550.878592   \n",
       "3                     297.476011           22.622249         307.367604   \n",
       "4                      48.950702           22.738509          50.512281   \n",
       "...                          ...                 ...                ...   \n",
       "2024                  320.696728           19.720443         331.596064   \n",
       "2025                  125.877778           19.052095         131.968254   \n",
       "2026                  361.450903           19.199717         372.257236   \n",
       "2027                  168.633173           22.766851         175.246154   \n",
       "2028                  363.040654           21.986609         375.305998   \n",
       "\n",
       "       ari_index    lix_index  dale_chall_score  dale_chall_known_fraction  \\\n",
       "0     530.109233  1087.007670         60.861630                   0.421860   \n",
       "1     257.398406   535.067729         33.517646                   0.454183   \n",
       "2     688.591738  1399.777126         75.341028                   0.513196   \n",
       "3     382.826867   794.121821         46.922592                   0.374833   \n",
       "4      66.598158   159.614035         14.380453                   0.447368   \n",
       "...          ...          ...               ...                        ...   \n",
       "2024  413.503801   852.237392         48.676215                   0.471095   \n",
       "2025  164.016000   350.555556         23.594190                   0.495238   \n",
       "2026  466.083993   953.167573         53.520354                   0.497280   \n",
       "2027  217.468053   460.951923         30.654177                   0.365385   \n",
       "2028  467.312617   959.639040         54.454400                   0.431843   \n",
       "\n",
       "      syllable_count  lexicon_count  sentence_count  \n",
       "0             1990.0         1042.0             1.0  \n",
       "1              877.0          502.0             1.0  \n",
       "2             2351.0         1364.0             1.0  \n",
       "3             1474.0          747.0             1.0  \n",
       "4              214.0          114.0             1.0  \n",
       "...              ...            ...             ...  \n",
       "2024          1439.0          813.0             1.0  \n",
       "2025           519.0          315.0             1.0  \n",
       "2026          1623.0          919.0             1.0  \n",
       "2027           847.0          416.0             1.0  \n",
       "2028          1769.0          917.0             1.0  \n",
       "\n",
       "[2029 rows x 46 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6b8aaf8",
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1657297507922,
     "user": {
      "displayName": "Francesca Spezzano",
      "userId": "01786709092490819085"
     },
     "user_tz": 360
    },
    "id": "14f005aa"
   },
   "outputs": [],
   "source": [
    "X = df_final.iloc[:, 0:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d0432d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1657297511158,
     "user": {
      "displayName": "Francesca Spezzano",
      "userId": "01786709092490819085"
     },
     "user_tz": 360
    },
    "id": "273bff46",
    "outputId": "3fffef56-b81c-470e-dfce-8607c0d779f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Objective</th>\n",
       "      <th>afraid</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch_kincaid_grade_level</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>ari_index</th>\n",
       "      <th>lix_index</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>dale_chall_known_fraction</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>lexicon_count</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.061233</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.068307</td>\n",
       "      <td>0.718545</td>\n",
       "      <td>0.187126</td>\n",
       "      <td>...</td>\n",
       "      <td>412.189204</td>\n",
       "      <td>21.736222</td>\n",
       "      <td>425.522148</td>\n",
       "      <td>530.109233</td>\n",
       "      <td>1087.007670</td>\n",
       "      <td>60.861630</td>\n",
       "      <td>0.421860</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011874</td>\n",
       "      <td>0.027143</td>\n",
       "      <td>0.017349</td>\n",
       "      <td>0.041963</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.059647</td>\n",
       "      <td>0.771184</td>\n",
       "      <td>0.145114</td>\n",
       "      <td>...</td>\n",
       "      <td>199.088805</td>\n",
       "      <td>18.940518</td>\n",
       "      <td>206.935458</td>\n",
       "      <td>257.398406</td>\n",
       "      <td>535.067729</td>\n",
       "      <td>33.517646</td>\n",
       "      <td>0.454183</td>\n",
       "      <td>877.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018336</td>\n",
       "      <td>0.037914</td>\n",
       "      <td>0.046958</td>\n",
       "      <td>0.066391</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.065443</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.055853</td>\n",
       "      <td>0.665447</td>\n",
       "      <td>0.216482</td>\n",
       "      <td>...</td>\n",
       "      <td>535.393607</td>\n",
       "      <td>19.220051</td>\n",
       "      <td>550.878592</td>\n",
       "      <td>688.591738</td>\n",
       "      <td>1399.777126</td>\n",
       "      <td>75.341028</td>\n",
       "      <td>0.513196</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.063752</td>\n",
       "      <td>0.031115</td>\n",
       "      <td>0.032674</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.081468</td>\n",
       "      <td>0.706590</td>\n",
       "      <td>0.218814</td>\n",
       "      <td>...</td>\n",
       "      <td>297.476011</td>\n",
       "      <td>22.622249</td>\n",
       "      <td>307.367604</td>\n",
       "      <td>382.826867</td>\n",
       "      <td>794.121821</td>\n",
       "      <td>46.922592</td>\n",
       "      <td>0.374833</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026660</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.042772</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.020359</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.039643</td>\n",
       "      <td>0.673569</td>\n",
       "      <td>0.205156</td>\n",
       "      <td>...</td>\n",
       "      <td>48.950702</td>\n",
       "      <td>22.738509</td>\n",
       "      <td>50.512281</td>\n",
       "      <td>66.598158</td>\n",
       "      <td>159.614035</td>\n",
       "      <td>14.380453</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>214.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.053275</td>\n",
       "      <td>0.016001</td>\n",
       "      <td>0.034158</td>\n",
       "      <td>0.059056</td>\n",
       "      <td>0.027101</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.706424</td>\n",
       "      <td>0.109309</td>\n",
       "      <td>...</td>\n",
       "      <td>320.696728</td>\n",
       "      <td>19.720443</td>\n",
       "      <td>331.596064</td>\n",
       "      <td>413.503801</td>\n",
       "      <td>852.237392</td>\n",
       "      <td>48.676215</td>\n",
       "      <td>0.471095</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>0.030234</td>\n",
       "      <td>0.034679</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.057117</td>\n",
       "      <td>0.039077</td>\n",
       "      <td>0.035642</td>\n",
       "      <td>0.021987</td>\n",
       "      <td>0.067945</td>\n",
       "      <td>0.704699</td>\n",
       "      <td>0.070619</td>\n",
       "      <td>...</td>\n",
       "      <td>125.877778</td>\n",
       "      <td>19.052095</td>\n",
       "      <td>131.968254</td>\n",
       "      <td>164.016000</td>\n",
       "      <td>350.555556</td>\n",
       "      <td>23.594190</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>519.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.102877</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.023160</td>\n",
       "      <td>0.021470</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0.116851</td>\n",
       "      <td>0.677108</td>\n",
       "      <td>0.105368</td>\n",
       "      <td>...</td>\n",
       "      <td>361.450903</td>\n",
       "      <td>19.199717</td>\n",
       "      <td>372.257236</td>\n",
       "      <td>466.083993</td>\n",
       "      <td>953.167573</td>\n",
       "      <td>53.520354</td>\n",
       "      <td>0.497280</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>0.017304</td>\n",
       "      <td>0.020901</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.013270</td>\n",
       "      <td>0.013475</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>0.810281</td>\n",
       "      <td>0.063775</td>\n",
       "      <td>...</td>\n",
       "      <td>168.633173</td>\n",
       "      <td>22.766851</td>\n",
       "      <td>175.246154</td>\n",
       "      <td>217.468053</td>\n",
       "      <td>460.951923</td>\n",
       "      <td>30.654177</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>847.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>0.029043</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.053885</td>\n",
       "      <td>0.034022</td>\n",
       "      <td>0.030847</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.077948</td>\n",
       "      <td>0.698336</td>\n",
       "      <td>0.068205</td>\n",
       "      <td>...</td>\n",
       "      <td>363.040654</td>\n",
       "      <td>21.986609</td>\n",
       "      <td>375.305998</td>\n",
       "      <td>467.312617</td>\n",
       "      <td>959.639040</td>\n",
       "      <td>54.454400</td>\n",
       "      <td>0.431843</td>\n",
       "      <td>1769.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Anger  Anticipation   Disgust      Fear       Joy   Sadness  \\\n",
       "0     0.011846      0.040958  0.031052  0.061233  0.023925  0.039174   \n",
       "1     0.011874      0.027143  0.017349  0.041963  0.023242  0.037897   \n",
       "2     0.018336      0.037914  0.046958  0.066391  0.030701  0.065443   \n",
       "3     0.012988      0.045399  0.021112  0.063752  0.031115  0.032674   \n",
       "4     0.026660      0.033526  0.042772  0.089552  0.020359  0.068824   \n",
       "...        ...           ...       ...       ...       ...       ...   \n",
       "2024  0.014073      0.053275  0.016001  0.034158  0.059056  0.027101   \n",
       "2025  0.030234      0.034679  0.008621  0.057117  0.039077  0.035642   \n",
       "2026  0.012072      0.102877  0.005359  0.023160  0.021470  0.025328   \n",
       "2027  0.017304      0.020901  0.007770  0.023442  0.013270  0.013475   \n",
       "2028  0.029043      0.042088  0.018604  0.053885  0.034022  0.030847   \n",
       "\n",
       "      Surprise     Trust  Objective    afraid  ...  \\\n",
       "0     0.004960  0.068307   0.718545  0.187126  ...   \n",
       "1     0.009701  0.059647   0.771184  0.145114  ...   \n",
       "2     0.012957  0.055853   0.665447  0.216482  ...   \n",
       "3     0.004902  0.081468   0.706590  0.218814  ...   \n",
       "4     0.005095  0.039643   0.673569  0.205156  ...   \n",
       "...        ...       ...        ...       ...  ...   \n",
       "2024  0.010837  0.079075   0.706424  0.109309  ...   \n",
       "2025  0.021987  0.067945   0.704699  0.070619  ...   \n",
       "2026  0.015776  0.116851   0.677108  0.105368  ...   \n",
       "2027  0.006283  0.087273   0.810281  0.063775  ...   \n",
       "2028  0.015228  0.077948   0.698336  0.068205  ...   \n",
       "\n",
       "      flesch_kincaid_grade_level  coleman_liau_index  gunning_fog_index  \\\n",
       "0                     412.189204           21.736222         425.522148   \n",
       "1                     199.088805           18.940518         206.935458   \n",
       "2                     535.393607           19.220051         550.878592   \n",
       "3                     297.476011           22.622249         307.367604   \n",
       "4                      48.950702           22.738509          50.512281   \n",
       "...                          ...                 ...                ...   \n",
       "2024                  320.696728           19.720443         331.596064   \n",
       "2025                  125.877778           19.052095         131.968254   \n",
       "2026                  361.450903           19.199717         372.257236   \n",
       "2027                  168.633173           22.766851         175.246154   \n",
       "2028                  363.040654           21.986609         375.305998   \n",
       "\n",
       "       ari_index    lix_index  dale_chall_score  dale_chall_known_fraction  \\\n",
       "0     530.109233  1087.007670         60.861630                   0.421860   \n",
       "1     257.398406   535.067729         33.517646                   0.454183   \n",
       "2     688.591738  1399.777126         75.341028                   0.513196   \n",
       "3     382.826867   794.121821         46.922592                   0.374833   \n",
       "4      66.598158   159.614035         14.380453                   0.447368   \n",
       "...          ...          ...               ...                        ...   \n",
       "2024  413.503801   852.237392         48.676215                   0.471095   \n",
       "2025  164.016000   350.555556         23.594190                   0.495238   \n",
       "2026  466.083993   953.167573         53.520354                   0.497280   \n",
       "2027  217.468053   460.951923         30.654177                   0.365385   \n",
       "2028  467.312617   959.639040         54.454400                   0.431843   \n",
       "\n",
       "      syllable_count  lexicon_count  sentence_count  \n",
       "0             1990.0         1042.0             1.0  \n",
       "1              877.0          502.0             1.0  \n",
       "2             2351.0         1364.0             1.0  \n",
       "3             1474.0          747.0             1.0  \n",
       "4              214.0          114.0             1.0  \n",
       "...              ...            ...             ...  \n",
       "2024          1439.0          813.0             1.0  \n",
       "2025           519.0          315.0             1.0  \n",
       "2026          1623.0          919.0             1.0  \n",
       "2027           847.0          416.0             1.0  \n",
       "2028          1769.0          917.0             1.0  \n",
       "\n",
       "[2029 rows x 46 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28eeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result['reliability']=final_result['reliability'].apply(lambda x:int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a919165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Objective</th>\n",
       "      <th>...</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>ari_index</th>\n",
       "      <th>lix_index</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>dale_chall_known_fraction</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>lexicon_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.061233</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.068307</td>\n",
       "      <td>0.718545</td>\n",
       "      <td>...</td>\n",
       "      <td>21.736222</td>\n",
       "      <td>425.522148</td>\n",
       "      <td>530.109233</td>\n",
       "      <td>1087.007670</td>\n",
       "      <td>60.861630</td>\n",
       "      <td>0.421860</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011874</td>\n",
       "      <td>0.027143</td>\n",
       "      <td>0.017349</td>\n",
       "      <td>0.041963</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.059647</td>\n",
       "      <td>0.771184</td>\n",
       "      <td>...</td>\n",
       "      <td>18.940518</td>\n",
       "      <td>206.935458</td>\n",
       "      <td>257.398406</td>\n",
       "      <td>535.067729</td>\n",
       "      <td>33.517646</td>\n",
       "      <td>0.454183</td>\n",
       "      <td>877.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.018336</td>\n",
       "      <td>0.037914</td>\n",
       "      <td>0.046958</td>\n",
       "      <td>0.066391</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.065443</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.055853</td>\n",
       "      <td>0.665447</td>\n",
       "      <td>...</td>\n",
       "      <td>19.220051</td>\n",
       "      <td>550.878592</td>\n",
       "      <td>688.591738</td>\n",
       "      <td>1399.777126</td>\n",
       "      <td>75.341028</td>\n",
       "      <td>0.513196</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.063752</td>\n",
       "      <td>0.031115</td>\n",
       "      <td>0.032674</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.081468</td>\n",
       "      <td>0.706590</td>\n",
       "      <td>...</td>\n",
       "      <td>22.622249</td>\n",
       "      <td>307.367604</td>\n",
       "      <td>382.826867</td>\n",
       "      <td>794.121821</td>\n",
       "      <td>46.922592</td>\n",
       "      <td>0.374833</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.026660</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.042772</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.020359</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.039643</td>\n",
       "      <td>0.673569</td>\n",
       "      <td>...</td>\n",
       "      <td>22.738509</td>\n",
       "      <td>50.512281</td>\n",
       "      <td>66.598158</td>\n",
       "      <td>159.614035</td>\n",
       "      <td>14.380453</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>214.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2024</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.053275</td>\n",
       "      <td>0.016001</td>\n",
       "      <td>0.034158</td>\n",
       "      <td>0.059056</td>\n",
       "      <td>0.027101</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.706424</td>\n",
       "      <td>...</td>\n",
       "      <td>19.720443</td>\n",
       "      <td>331.596064</td>\n",
       "      <td>413.503801</td>\n",
       "      <td>852.237392</td>\n",
       "      <td>48.676215</td>\n",
       "      <td>0.471095</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>2025</td>\n",
       "      <td>0.030234</td>\n",
       "      <td>0.034679</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.057117</td>\n",
       "      <td>0.039077</td>\n",
       "      <td>0.035642</td>\n",
       "      <td>0.021987</td>\n",
       "      <td>0.067945</td>\n",
       "      <td>0.704699</td>\n",
       "      <td>...</td>\n",
       "      <td>19.052095</td>\n",
       "      <td>131.968254</td>\n",
       "      <td>164.016000</td>\n",
       "      <td>350.555556</td>\n",
       "      <td>23.594190</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>519.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>2026</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.102877</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.023160</td>\n",
       "      <td>0.021470</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0.116851</td>\n",
       "      <td>0.677108</td>\n",
       "      <td>...</td>\n",
       "      <td>19.199717</td>\n",
       "      <td>372.257236</td>\n",
       "      <td>466.083993</td>\n",
       "      <td>953.167573</td>\n",
       "      <td>53.520354</td>\n",
       "      <td>0.497280</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>2027</td>\n",
       "      <td>0.017304</td>\n",
       "      <td>0.020901</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>0.023442</td>\n",
       "      <td>0.013270</td>\n",
       "      <td>0.013475</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>0.810281</td>\n",
       "      <td>...</td>\n",
       "      <td>22.766851</td>\n",
       "      <td>175.246154</td>\n",
       "      <td>217.468053</td>\n",
       "      <td>460.951923</td>\n",
       "      <td>30.654177</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>847.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>2028</td>\n",
       "      <td>0.029043</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.053885</td>\n",
       "      <td>0.034022</td>\n",
       "      <td>0.030847</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.077948</td>\n",
       "      <td>0.698336</td>\n",
       "      <td>...</td>\n",
       "      <td>21.986609</td>\n",
       "      <td>375.305998</td>\n",
       "      <td>467.312617</td>\n",
       "      <td>959.639040</td>\n",
       "      <td>54.454400</td>\n",
       "      <td>0.431843</td>\n",
       "      <td>1769.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id     Anger  Anticipation   Disgust      Fear       Joy   Sadness  \\\n",
       "0           0  0.011846      0.040958  0.031052  0.061233  0.023925  0.039174   \n",
       "1           1  0.011874      0.027143  0.017349  0.041963  0.023242  0.037897   \n",
       "2           2  0.018336      0.037914  0.046958  0.066391  0.030701  0.065443   \n",
       "3           3  0.012988      0.045399  0.021112  0.063752  0.031115  0.032674   \n",
       "4           4  0.026660      0.033526  0.042772  0.089552  0.020359  0.068824   \n",
       "...       ...       ...           ...       ...       ...       ...       ...   \n",
       "2024     2024  0.014073      0.053275  0.016001  0.034158  0.059056  0.027101   \n",
       "2025     2025  0.030234      0.034679  0.008621  0.057117  0.039077  0.035642   \n",
       "2026     2026  0.012072      0.102877  0.005359  0.023160  0.021470  0.025328   \n",
       "2027     2027  0.017304      0.020901  0.007770  0.023442  0.013270  0.013475   \n",
       "2028     2028  0.029043      0.042088  0.018604  0.053885  0.034022  0.030847   \n",
       "\n",
       "      Surprise     Trust  Objective  ...  coleman_liau_index  \\\n",
       "0     0.004960  0.068307   0.718545  ...           21.736222   \n",
       "1     0.009701  0.059647   0.771184  ...           18.940518   \n",
       "2     0.012957  0.055853   0.665447  ...           19.220051   \n",
       "3     0.004902  0.081468   0.706590  ...           22.622249   \n",
       "4     0.005095  0.039643   0.673569  ...           22.738509   \n",
       "...        ...       ...        ...  ...                 ...   \n",
       "2024  0.010837  0.079075   0.706424  ...           19.720443   \n",
       "2025  0.021987  0.067945   0.704699  ...           19.052095   \n",
       "2026  0.015776  0.116851   0.677108  ...           19.199717   \n",
       "2027  0.006283  0.087273   0.810281  ...           22.766851   \n",
       "2028  0.015228  0.077948   0.698336  ...           21.986609   \n",
       "\n",
       "      gunning_fog_index   ari_index    lix_index  dale_chall_score  \\\n",
       "0            425.522148  530.109233  1087.007670         60.861630   \n",
       "1            206.935458  257.398406   535.067729         33.517646   \n",
       "2            550.878592  688.591738  1399.777126         75.341028   \n",
       "3            307.367604  382.826867   794.121821         46.922592   \n",
       "4             50.512281   66.598158   159.614035         14.380453   \n",
       "...                 ...         ...          ...               ...   \n",
       "2024         331.596064  413.503801   852.237392         48.676215   \n",
       "2025         131.968254  164.016000   350.555556         23.594190   \n",
       "2026         372.257236  466.083993   953.167573         53.520354   \n",
       "2027         175.246154  217.468053   460.951923         30.654177   \n",
       "2028         375.305998  467.312617   959.639040         54.454400   \n",
       "\n",
       "      dale_chall_known_fraction  syllable_count  lexicon_count  \\\n",
       "0                      0.421860          1990.0         1042.0   \n",
       "1                      0.454183           877.0          502.0   \n",
       "2                      0.513196          2351.0         1364.0   \n",
       "3                      0.374833          1474.0          747.0   \n",
       "4                      0.447368           214.0          114.0   \n",
       "...                         ...             ...            ...   \n",
       "2024                   0.471095          1439.0          813.0   \n",
       "2025                   0.495238           519.0          315.0   \n",
       "2026                   0.497280          1623.0          919.0   \n",
       "2027                   0.365385           847.0          416.0   \n",
       "2028                   0.431843          1769.0          917.0   \n",
       "\n",
       "      sentence_count  reliability  \n",
       "0                1.0            1  \n",
       "1                1.0            1  \n",
       "2                1.0            1  \n",
       "3                1.0            0  \n",
       "4                1.0            1  \n",
       "...              ...          ...  \n",
       "2024             1.0            0  \n",
       "2025             1.0            0  \n",
       "2026             1.0            0  \n",
       "2027             1.0            0  \n",
       "2028             1.0            0  \n",
       "\n",
       "[2029 rows x 48 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c1c0896",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1657297516225,
     "user": {
      "displayName": "Francesca Spezzano",
      "userId": "01786709092490819085"
     },
     "user_tz": 360
    },
    "id": "43f9bba5",
    "outputId": "3075ec0c-e61e-48f1-a63a-31c17fc7b967"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "2024    0\n",
       "2025    0\n",
       "2026    0\n",
       "2027    0\n",
       "2028    0\n",
       "Name: reliability, Length: 2029, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = final_result.iloc[:,-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da630c12",
   "metadata": {
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1657297518441,
     "user": {
      "displayName": "Francesca Spezzano",
      "userId": "01786709092490819085"
     },
     "user_tz": 360
    },
    "id": "f1722641"
   },
   "outputs": [],
   "source": [
    "y=np.array([x for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4932e115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1657297519881,
     "user": {
      "displayName": "Francesca Spezzano",
      "userId": "01786709092490819085"
     },
     "user_tz": 360
    },
    "id": "d410d34c",
    "outputId": "7f229a64-067c-4ed7-8bff-1d75c2733dac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75dd6b",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdaa6d95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35564,
     "status": "ok",
     "timestamp": 1657297637179,
     "user": {
      "displayName": "Francesca Spezzano",
      "userId": "01786709092490819085"
     },
     "user_tz": 360
    },
    "id": "26d4657a",
    "outputId": "05e15f32-be4e-44f7-de43-2d5059a45402"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:43] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:44] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:44] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:45] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:45] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:46] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:46] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:47] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:47] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:48] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "AUROC:  0.8280680063129452\n",
      "Avg Precision:  0.898236086105572\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "avgP=0\n",
    "avgAUROC=0\n",
    "cv=5\n",
    "skf = StratifiedKFold(n_splits=cv,random_state=10,shuffle=True)\n",
    "for train_index, val_index in skf.split(X.values, y):\n",
    "  X_train1=X.values[train_index]\n",
    "  X_val=X.values[val_index]\n",
    "  y_train1=y[train_index]\n",
    "  y_val=y[val_index]\n",
    "  clf=XGBClassifier(objective=\"binary:logistic\", random_state=100)\n",
    "  clf.fit(X_train1,y_train1)\n",
    "  y_pred_val=clf.predict_proba(X_val)[:,1]\n",
    "  #print(y_pred_val)\n",
    "  avgP+=average_precision_score(y_val, y_pred_val)\n",
    "  #print(average_precision_score(y_val, y_pred_val))\n",
    "  avgAUROC+=roc_auc_score(y_val, y_pred_val)\n",
    "  #print(roc_auc_score(y_val, y_pred_val))\n",
    "avgP=avgP/cv\n",
    "avgROC=avgAUROC/cv\n",
    "print('AUROC: ',avgROC)\n",
    "print('Avg Precision: ',avgP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128c592",
   "metadata": {
    "id": "3ea95774"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d2d6126",
   "metadata": {
    "id": "10619461",
    "outputId": "2439bd99-aa45-401f-ed64-9551a0b1a5b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC:  0.7683401700761278\n",
      "Avg Precision:  0.8554235987916018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "C=[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000,1000,10000000]\n",
    "bestC=0\n",
    "bestValue=-1\n",
    "cv=5\n",
    "for c in C:\n",
    "    avgP=0\n",
    "    avgAUROC=0\n",
    "    skf = StratifiedKFold(n_splits=cv,random_state=10,shuffle=True )\n",
    "    for train_index, val_index in skf.split(X.values, y):\n",
    "        X_train1=X.values[train_index]\n",
    "        X_val=X.values[val_index]\n",
    "        y_train1=y[train_index]\n",
    "        y_val=y[val_index]\n",
    "        clf=RandomForestClassifier(max_depth=2, random_state=0)\n",
    "        clf.fit(X_train1,y_train1)\n",
    "        y_pred_val=clf.predict_proba(X_val)[:,1]\n",
    "        #print(y_pred_val)\n",
    "        avgP+=average_precision_score(y_val, y_pred_val)\n",
    "        avgAUROC+=roc_auc_score(y_val, y_pred_val)\n",
    "    avgP=avgP/cv\n",
    "    avgROC=avgAUROC/cv\n",
    "print('AUROC: ',avgROC)\n",
    "print('Avg Precision: ',avgP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9315e2",
   "metadata": {
    "id": "d7387de1"
   },
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98570dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC:  0.7920789570889084\n",
      "Avg Precision:  0.8610846985007553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "C=[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000,1000,10000000]\n",
    "bestC=0\n",
    "bestValue=-1\n",
    "cv=5\n",
    "for c in C:\n",
    "    avgP=0\n",
    "    avgAUROC=0\n",
    "    skf = StratifiedKFold(n_splits=cv,random_state=10,shuffle=True )\n",
    "    for train_index, val_index in skf.split(X.values, y):\n",
    "        X_train1=X.values[train_index]\n",
    "        X_val=X.values[val_index]\n",
    "        y_train1=y[train_index]\n",
    "        y_val=y[val_index]\n",
    "        clf=LogisticRegression(random_state=0, class_weight='balanced',C=c,penalty='l2')\n",
    "        clf.fit(X_train1,y_train1)\n",
    "        y_pred_val=clf.predict_proba(X_val)[:,1]\n",
    "        #print(y_pred_val)\n",
    "        avgP+=average_precision_score(y_val, y_pred_val)\n",
    "        avgAUROC+=roc_auc_score(y_val, y_pred_val)\n",
    "    avgP=avgP/cv\n",
    "    avgROC=avgAUROC/cv\n",
    "print('AUROC: ',avgROC)\n",
    "print('Avg Precision: ',avgP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e1cc7",
   "metadata": {},
   "source": [
    "# Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98200ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC:  0.8159700924904374\n",
      "Avg Precision:  0.8952898267006366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "C=[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000,1000,10000000]\n",
    "bestC=0\n",
    "bestValue=-1\n",
    "cv=5\n",
    "for c in C:\n",
    "    avgP=0\n",
    "    avgAUROC=0\n",
    "    skf = StratifiedKFold(n_splits=cv,random_state=10,shuffle=True )\n",
    "    for train_index, val_index in skf.split(X.values, y):\n",
    "        X_train1=X.values[train_index]\n",
    "        X_val=X.values[val_index]\n",
    "        y_train1=y[train_index]\n",
    "        y_val=y[val_index]\n",
    "        clf=ExtraTreesClassifier(ccp_alpha=0.0,class_weight='balanced',n_estimators=100, random_state=100)\n",
    "        clf.fit(X_train1,y_train1)\n",
    "        y_pred_val=clf.predict_proba(X_val)[:,1]\n",
    "        #print(y_pred_val)\n",
    "        avgP+=average_precision_score(y_val, y_pred_val)\n",
    "        avgAUROC+=roc_auc_score(y_val, y_pred_val)\n",
    "    avgP=avgP/cv\n",
    "    avgROC=avgAUROC/cv\n",
    "print('AUROC: ',avgROC)\n",
    "print('Avg Precision: ',avgP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e61ff",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0fb708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC:  0.5833228433073634\n",
      "Avg Precision:  0.713399063520361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.svm import LinearSVC\n",
    "C=[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100,500,1000,1000,10000000]\n",
    "bestC=0\n",
    "bestValue=-1\n",
    "cv=5\n",
    "for c in C:\n",
    "    avgP=0\n",
    "    avgAUROC=0\n",
    "    skf = StratifiedKFold(n_splits=cv,random_state=10,shuffle=True )\n",
    "    for train_index, val_index in skf.split(X.values, y):\n",
    "        X_train1=X.values[train_index]\n",
    "        X_val=X.values[val_index]\n",
    "        y_train1=y[train_index]\n",
    "        y_val=y[val_index]\n",
    "        clf=LinearSVC(class_weight='balanced',random_state=100),\n",
    "        clf[0].fit(X_train1,y_train1)\n",
    "        y_pred_val=clf[0].predict(X_val)\n",
    "        #print(y_pred_val)\n",
    "        avgP+=average_precision_score(y_val, y_pred_val)\n",
    "        avgAUROC+=roc_auc_score(y_val, y_pred_val)\n",
    "    avgP=avgP/cv\n",
    "    avgROC=avgAUROC/cv\n",
    "print('AUROC: ',avgROC)\n",
    "print('Avg Precision: ',avgP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d714d11",
   "metadata": {
    "id": "17aaffb3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AvgP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.861085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.583323</td>\n",
       "      <td>0.713399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.768340</td>\n",
       "      <td>0.855424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.815970</td>\n",
       "      <td>0.895290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.828068</td>\n",
       "      <td>0.898236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier     AUROC      AvgP\n",
       "0  Logistic Regression  0.792079  0.861085\n",
       "1                  SVM  0.583323  0.713399\n",
       "2        Random Forest  0.768340  0.855424\n",
       "3          Extra Trees  0.815970  0.895290\n",
       "4              XGBoost  0.828068  0.898236"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif = ['Logistic Regression','SVM','Random Forest','Extra Trees','XGBoost']\n",
    "\n",
    "classifier_real_network_1 = pd.DataFrame(\n",
    "    {'Classifier': classif,\n",
    "     'AUROC': [0.7920789570889084,0.5833228433073634,0.7683401700761278,0.8159700924904374,0.8280680063129452],\n",
    "     'AvgP': [0.8610846985007553,0.713399063520361,0.8554235987916018,0.8952898267006366,0.898236086105572],\n",
    "    })\n",
    "\n",
    "classifier_real_network_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "114adae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv(\"allfcombine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559c876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Test_code_for_Abishi_Paper_francesca.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
